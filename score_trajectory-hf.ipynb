{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaef82d1-876a-488b-8df6-04c764990b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from getpass import getpass \n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from fastchat.conversation import get_conv_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e68e8e3-ceb5-4e86-adc0-154dc43d380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {}\n",
    "messages['instruction'] = \"\"\"### Task Description: An instruction (might include an Input inside it), a response to evaluate and a score rubric representing a evaluation criteria are given. \n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. \n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. \n",
    "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" \n",
    "4. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "### Instruction to evaluate:\n",
    "The instruction to evaluate:  <history><HISTORY_HERE></history>\n",
    "\n",
    "### Response to evaluate:\n",
    "Agent's response: <response><AGENT_RESPONSE_TO_SCORE></response>\n",
    "User's feedback: <user><USER_RESPONSE></user>\n",
    "\n",
    "### Score Rubric: [Evaluating the Model's Progression Towards Task Completion]\n",
    "Score 1: The model completely fails to function as a multi-turn agent, as shown by it taking multiple actions in a single response.\n",
    "Score 2: The model entirely fails to adhere to the correct action syntax as directed by the user, utilizing incorrect action formats.\n",
    "Score 3: The model does not acknowledge its previous errors (if any) and repeats actions. Alternatively, the model executes an incorrect action.\n",
    "Score 4: The model perfectly executes the correct action or accurately acknowledges its past mistakes (if any).\n",
    "Score 5: The model not only executes the correct action flawlessly but also delivers engaging responses that are specifically tailored to the user's tone and level of understanding.\n",
    "\n",
    "### Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6f798f-33d0-4a90-8dc2-ce5a48944709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c924fcb59554180a512b81f1e6adf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"kaist-ai/Prometheus-13b-v1.0\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be717cff-8c68-436e-8650-9fcf6d02764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path, idx = \"./data/os-interaction/Mistral-7B-v0.1.jsonl\", 1\n",
    "\n",
    "conversations = [] \n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "    conversations.append(json_object)\n",
    "trajectory = conversations[index]['output']['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da950bcd-9399-4627-94d4-5f666725be0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "### Task Description: An instruction (might include an Input inside it), a response to evaluate and a score rubric representing a evaluation criteria are given. \n",
      "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. \n",
      "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. \n",
      "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" \n",
      "4. Please do not generate any other opening, closing, and explanations. \n",
      "\n",
      "### Instruction to evaluate:\n",
      "The instruction to evaluate:  <history>User: You are an assistant that will act like a person, I'will play the role of linux(ubuntu) operating system. Your goal is to implement the operations required by me or answer to the question proposed by me. For each of your turn, you should first think what you should do, and then take exact one of the three actions: \"bash\", \"finish\" or \"answer\". \n",
      "\n",
      "1. If you think you should execute some bash code, take bash action, and you should print like this:\n",
      "\n",
      "Think: put your thought here.\n",
      "\n",
      "Act: bash\n",
      "\n",
      "```bash\n",
      "# put your bash code here\n",
      "```\n",
      "\n",
      "2. If you think you have finished the task, take finish action, and you should print like this:\n",
      "\n",
      "Think: put your thought here.\n",
      "\n",
      "Act: finish\n",
      "\n",
      "3. If you think you have got the answer to the question, take answer action, and you should print like this:\n",
      "\n",
      "Think: put your thought here.\n",
      "\n",
      "Act: answer(Your answer to the question should be put in this pair of parentheses)\n",
      "\n",
      "If the output is too long, I will truncate it. The truncated output is not complete. You have to deal with the truncating problem by yourself. Attention, your bash code should not contain any input operation. Once again, you should take only exact one of the three actions in each turn.\n",
      "\n",
      "Now, my problem is:\n",
      "\n",
      "tell me how many files are in the directory \"/etc\"?</history>\n",
      "\n",
      "### Response to evaluate:\n",
      "Agent's response: <response>Think: To count the files in /etc, I need to print all the files in it.\n",
      "\n",
      "Act: bash\n",
      "\n",
      "```bash\n",
      "ls /etc\n",
      "```</response>\n",
      "User's feedback: <user>The output of the OS:\n",
      "cpi cron.hourly fuse.conf iproute2 lvm networkd-dispatcher protocols selinux tmpfiles.d [truncated because the output is too long]</user>\n",
      "\n",
      "### Score Rubric: [Evaluating the Model's Progression Towards Task Completion]\n",
      "Score 1: The model completely fails to function as a multi-turn agent, as shown by it taking multiple actions in a single response.\n",
      "Score 2: The model entirely fails to adhere to the correct action syntax as directed by the user, utilizing incorrect action formats.\n",
      "Score 3: The model does not acknowledge its previous errors (if any) and repeats actions. Alternatively, the model executes an incorrect action.\n",
      "Score 4: The model perfectly executes the correct action or accurately acknowledges its past mistakes (if any).\n",
      "Score 5: The model not only executes the correct action flawlessly but also delivers engaging responses that are specifically tailored to the user's tone and level of understanding.\n",
      "\n",
      "### Feedback:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[INST] <<SYS>>\n",
      "You are a fair evaluator language model.\n",
      "<</SYS>>\n",
      "\n",
      "### Task Description: An instruction (might include an Input inside it), a response to evaluate and a score rubric representing a evaluation criteria are given. \n",
      "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. \n",
      "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. \n",
      "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" \n",
      "4. Please do not generate any other opening, closing, and explanations. \n",
      "\n",
      "### Instruction to evaluate:\n",
      "The instruction to evaluate:  <history>User: You are an assistant that will act like a person, I'will play the role of linux(ubuntu) operating system. Your goal is to implement the operations required by me or answer to the question proposed by me. For each of your turn, you should first think what you should do, and then take exact one of the three actions: \"bash\", \"finish\" or \"answer\". \n",
      "\n",
      "1. If you think you should execute some bash code, take bash action, and you should print like this:\n",
      "\n",
      "Think: put your thought here.\n",
      "\n",
      "Act: bash\n",
      "\n",
      "```bash\n",
      "# put your bash code here\n",
      "```\n",
      "\n",
      "2. If you think you have finished the task, take finish action, and you should print like this:\n",
      "\n",
      "Think: put your thought here.\n",
      "\n",
      "Act: finish\n",
      "\n",
      "3. If you think you have got the answer to the question, take answer action, and you should print like this:\n",
      "\n",
      "Think: put your thought here.\n",
      "\n",
      "Act: answer(Your answer to the question should be put in this pair of parentheses)\n",
      "\n",
      "If the output is too long, I will truncate it. The truncated output is not complete. You have to deal with the truncating problem by yourself. Attention, your bash code should not contain any input operation. Once again, you should take only exact one of the three actions in each turn.\n",
      "\n",
      "Now, my problem is:\n",
      "\n",
      "tell me how many files are in the directory \"/etc\"?</history>\n",
      "\n",
      "### Response to evaluate:\n",
      "Agent's response: <response>Think: To count the files in /etc, I need to print all the files in it.\n",
      "\n",
      "Act: bash\n",
      "\n",
      "```bash\n",
      "ls /etc\n",
      "```</response>\n",
      "User's feedback: <user>The output of the OS:\n",
      "cpi cron.hourly fuse.conf iproute2 lvm networkd-dispatcher protocols selinux tmpfiles.d [truncated because the output is too long]</user>\n",
      "\n",
      "### Score Rubric: [Evaluating the Model's Progression Towards Task Completion]\n",
      "Score 1: The model completely fails to function as a multi-turn agent, as shown by it taking multiple actions in a single response.\n",
      "Score 2: The model entirely fails to adhere to the correct action syntax as directed by the user, utilizing incorrect action formats.\n",
      "Score 3: The model does not acknowledge its previous errors (if any) and repeats actions. Alternatively, the model executes an incorrect action.\n",
      "Score 4: The model perfectly executes the correct action or accurately acknowledges its past mistakes (if any).\n",
      "Score 5: The model not only executes the correct action flawlessly but also delivers engaging responses that are specifically tailored to the user's tone and level of understanding.\n",
      "\n",
      "### Feedback: [/INST]  Based on the provided response, the model executed an incorrect action, which results in a score of 3 according to the score rubric. The instruction clearly stated that the model should not perform an input operation, yet the model's response included the 'ls' command which is an input operation. This is a clear deviation from the instructions and shows a lack of understanding or carelessness on the model's part. Additionally, the model did not attempt to acknowledge or correct its mistake. This indicates that the model is repeating an action without learning from its previous errors. So the overall score is 3. [RESULT] 3\n"
     ]
    }
   ],
   "source": [
    "history = [f\"User: {ds['content'].strip()}\\n\" if ds['role'] == 'user' else f\"Agent: {ds['content'].strip()}\\n\" for ds in trajectory[:idx]]\n",
    "history = '\\n'.join(history).strip()\n",
    "response = trajectory[idx]['content']\n",
    "user = trajectory[idx+1]['content'] if idx < len(trajectory)-1 else ' '\n",
    "\n",
    "messages['instruction'] = messages['instruction'].replace(\"<HISTORY_HERE>\", history)\n",
    "messages['instruction'] = messages['instruction'].replace(\"<AGENT_RESPONSE_TO_SCORE>\", response)\n",
    "messages['instruction'] = messages['instruction'].replace(\"<USER_RESPONSE>\", user)\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(messages['instruction'])\n",
    "print(\"-\"*100)\n",
    "\n",
    "conv = get_conv_template(\"llama-2\")\n",
    "conv.set_system_message(\"You are a fair evaluator language model.\")\n",
    "conv.append_message(conv.roles[0], messages['instruction'])\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer(prompt, truncation=False, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output = model.generate(input_ids, do_sample=True, temperature=1.0, top_p=0.9, max_new_tokens=512, repetition_penalty=1.03)\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the decoded text\n",
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
